"""
Quick Red Team Test Suite for Week 3 Infrastructure

Tests edge cases and potential crashes.
"""

import sys
from pathlib import Path

sys.path.insert(0, str(Path(__file__).parent))

def test_imports():
    """Test all imports work."""
    print("üß™ Test 1: Imports")
    try:
        from src.evaluation.ab_testing import ABTester, ExperimentConfig
        from src.rag.pipeline import RAGPipeline
        from experiments.run_full_optimization import OptimizationRunner
        print("   ‚úÖ All imports successful\n")
        return True
    except Exception as e:
        print(f"   ‚ùå Import failed: {e}\n")
        return False


def test_dataset_validation():
    """Test dataset validation catches bad data."""
    print("üß™ Test 2: Dataset Validation")
    from src.evaluation.ab_testing import ABTester
    import tempfile
    import json
    
    # Test 1: Missing file
    try:
        tester = ABTester("nonexistent.json")
        print("   ‚ùå Should have raised FileNotFoundError")
        return False
    except FileNotFoundError:
        print("   ‚úÖ Correctly rejected missing file")
    
    # Test 2: Invalid JSON
    with tempfile.NamedTemporaryFile(mode='w', suffix='.json', delete=False) as f:
        f.write("{invalid json")
        temp_path = f.name
    
    try:
        tester = ABTester(temp_path)
        print("   ‚ùå Should have raised ValueError for invalid JSON")
        return False
    except ValueError as e:
        print(f"   ‚úÖ Correctly rejected invalid JSON: {str(e)[:50]}...")
    finally:
        Path(temp_path).unlink()
    
    # Test 3: Missing 'questions' field
    with tempfile.NamedTemporaryFile(mode='w', suffix='.json', delete=False) as f:
        json.dump({"data": []}, f)
        temp_path = f.name
    
    try:
        tester = ABTester(temp_path)
        print("   ‚ùå Should have raised ValueError for missing 'questions'")
        return False
    except ValueError as e:
        print(f"   ‚úÖ Correctly rejected missing 'questions': {str(e)[:50]}...")
    finally:
        Path(temp_path).unlink()
    
    # Test 4: Empty questions
    with tempfile.NamedTemporaryFile(mode='w', suffix='.json', delete=False) as f:
        json.dump({"questions": []}, f)
        temp_path = f.name
    
    try:
        tester = ABTester(temp_path)
        print("   ‚ùå Should have raised ValueError for empty questions")
        return False
    except ValueError:
        print("   ‚úÖ Correctly rejected empty questions array")
    finally:
        Path(temp_path).unlink()
    
    print()
    return True


def test_rag_pipeline_configuration():
    """Test RAG pipeline accepts configurations."""
    print("üß™ Test 3: RAG Pipeline Configuration")
    from src.rag.pipeline import RAGPipeline
    from src.evaluation.ab_testing import ExperimentConfig
    
    try:
        # Initialize (may fail if no vector DB, that's ok)
        rag = RAGPipeline()
        
        # Test configure method exists
        config = ExperimentConfig(
            name='test',
            retrieval_method='hybrid',
            chunk_size=512,
            top_k=5,
            alpha=0.5,
            prompt_template='default'
        )
        
        rag.configure(config)
        print("   ‚úÖ Configuration applied successfully\n")
        return True
    except Exception as e:
        print(f"   ‚ö†Ô∏è  Error (may be expected if no DB): {e}\n")
        return True  # Don't fail test if no DB


def test_experiment_config():
    """Test ExperimentConfig dataclass."""
    print("üß™ Test 4: ExperimentConfig")
    from src.evaluation.ab_testing import ExperimentConfig
    from dataclasses import asdict
    
    try:
        config = ExperimentConfig(
            name='test_config',
            retrieval_method='vector',
            chunk_size=256,
            top_k=3,
            alpha=1.0,
            prompt_template='concise'
        )
        
        # Test serialization
        config_dict = asdict(config)
        assert config_dict['chunk_size'] == 256
        assert config_dict['top_k'] == 3
        
        print("   ‚úÖ ExperimentConfig works correctly\n")
        return True
    except Exception as e:
        print(f"   ‚ùå ExperimentConfig failed: {e}\n")
        return False


def test_comparison_edge_cases():
    """Test comparison handles edge cases."""
    print("üß™ Test 5: Comparison Edge Cases")
    import statistics
    
    try:
        # Test empty list
        try:
            stats.mean([])
            print("   ‚ùå statistics.mean([]) should raise error")
            return False
        except statistics.StatisticsError:
            print("   ‚úÖ Empty list raises StatisticsError (expected)")
        
        # Test single value
        mean = statistics.mean([1.0])
        assert mean == 1.0
        print("   ‚úÖ Single value works")
        
        # Test all zeros
        mean = statistics.mean([0.0, 0.0, 0.0])
        assert mean == 0.0
        print("   ‚úÖ All zeros works")
        
        print()
        return True
    except Exception as e:
        print(f"   ‚ùå Edge case test failed: {e}\n")
        return False


def run_all_tests():
    """Run all red team tests."""
    print("="*70)
    print(" üî¥ RED TEAM TEST SUITE - Week 3 Infrastructure")
    print("="*70)
    print()
    
    tests = [
        test_imports,
        test_dataset_validation,
        test_rag_pipeline_configuration,
        test_experiment_config,
        test_comparison_edge_cases
    ]
    
    results = []
    for test in tests:
        try:
            results.append(test())
        except Exception as e:
            print(f"   üí• Unhandled exception in {test.__name__}: {e}\n")
            results.append(False)
    
    print("="*70)
    print(" üìä TEST RESULTS")
    print("="*70)
    passed = sum(results)
    total = len(results)
    print(f"Passed: {passed}/{total} ({passed/total*100:.0f}%)")
    print()
    
    if passed == total:
        print("‚úÖ ALL TESTS PASSED!")
        print("Week 3 infrastructure is robust!")
    else:
        failed = total - passed
        print(f"‚ö†Ô∏è  {failed} test(s) failed")
        print("Review failures above and fix issues")
    
    return passed == total


if __name__ == "__main__":
    success = run_all_tests()
    sys.exit(0 if success else 1)
