# üêõ Week 3 Red Team Assessment - Bug Report

**Date:** 2024-12-19 10:06 WIB  
**Scope:** A/B Testing Framework, RAG Pipeline, Experiment Runner  
**Status:** IN PROGRESS

---

## üî¥ CRITICAL BUGS (Breaks functionality)

### Bug #1: Undefined Variable `confidence` ‚ö†Ô∏è
**File:** `src/rag/pipeline.py:228`  
**Status:** FOUND  
**Severity:** CRITICAL  

**Issue:**
```python
# Line 223-230
return {
    'answer': answer,
    'sources': sources,
    'contexts': contexts,
    'retrieved_chunks': chunks,
    'confidence': confidence,  # ‚ùå UNDEFINED!
    'model_used': model_used,
    'tokens_used': self.last_token_count
}
```

**Error:**
```
NameError: name 'confidence' is not defined
```

**Root Cause:**
- Confidence calculation was accidentally removed during code editing
- Line should exist before return statement

**Fix:**
```python
# Add before return (around line 217):
confidence = sum(c['score'] for c in chunks) / len(chunks) if chunks else 0.0
```

**Impact:** RAG pipeline crashes on every query

**Priority:** FIX IMMEDIATELY ‚ö°

---

## üü° MEDIUM BUGS (Degraded functionality)

### Bug #2: Missing BM25 Initialization Check
**File:** `experiments/run_full_optimization.py`  
**Severity:** MEDIUM

**Issue:**
Experiment runner doesn't verify BM25 is available before running BM25/Hybrid experiments

**Potential Error:**
```python
# If BM25 not initialized, hybrid_search will fail silently
results = self.vector_store.hybrid_search(query, n_results, alpha=0.0)
# Could return empty or crash
```

**Fix:**
Add BM25 availability check in experiment setup

---

### Bug #3: No Dataset Validation
**File:** `src/evaluation/ab_testing.py:41`  
**Severity:** MEDIUM

**Issue:**
```python
def _load_dataset(self, path: str) -> List[Dict]:
    with open(path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    return data.get('questions', [])
    # ‚ùå No validation if 'questions' exists
    # ‚ùå No check if questions have required fields
```

**Risk:** Silent failures if dataset malformed

**Fix:**
```python
questions = data.get('questions')
if not questions:
    raise ValueError(f"No 'questions' found in {path}")
    
# Validate first question has required fields
if questions:
    required = ['id', 'question', 'ground_truth_answer']
    if not all(k in questions[0] for k in required):
        raise ValueError(f"Questions missing required fields: {required}")
```

---

### Bug #4: Experiment Limit Not Passed Through
**File:** `experiments/run_full_optimization.py:74`  
**Severity:** MEDIUM

**Issue:**
```python
results[config.name] = self.tester.run_experiment(
    config,
    self.rag,
    limit=10  # ‚ùå HARDCODED! Should use self.limit
)
```

**Impact:** User can't control test size from CLI

**Fix:**
```python
# In __init__:
def __init__(self, dataset_path, limit=None):
    self.limit = limit
    
# In experiments:
limit=self.limit  # Use instance variable
```

---

## üü¢ LOW PRIORITY (Edge cases)

### Bug #5: Division by Zero in Normalization
**File:** `src/retrieval/vector_search.py:222-226`  
**Severity:** LOW

**Issue:**
```python
max_semantic = max(semantic_scores.values()) if semantic_scores else 1.0
max_bm25 = max(bm25_scores.values()) if bm25_scores else 1.0

normalized_semantic = {k: v / max_semantic for k, v in semantic_scores.items()}
# If max_semantic = 0, division by zero!
```

**Edge Case:** All scores are 0.0

**Fix:**
```python
max_semantic = max(semantic_scores.values()) if semantic_scores else 1.0
max_semantic = max_semantic if max_semantic > 0 else 1.0  # Prevent division by zero
```

---

### Bug #6: No Empty Result Handling in AB Testing
**File:** `src/evaluation/ab_testing.py:162`  
**Severity:** LOW

**Issue:**
```python
mean_a = statistics.mean(values_a)
mean_b = statistics.mean(values_b)
# statistics.mean([]) raises StatisticsError
```

**Fix:**
```python
if not values_a or not values_b:
    print(f"  Skipping {metric}: insufficient data")
    continue
```

---

## üîí SECURITY ISSUES

### Sec #1: No API Key Validation
**File:** `src/rag/pipeline.py:61`  
**Severity:** MEDIUM

**Issue:**
```python
api_key = os.getenv("GEMINI_API_KEY")
if not api_key:
    print("‚ö†Ô∏è  GEMINI_API_KEY not found")
    # ‚ùå Continues anyway, sets self.llm = None
```

**Risk:** 
- Silent failure
- Experiments run without LLM, waste time

**Fix:**
```python
if not api_key:
    raise ValueError("GEMINI_API_KEY required in .env file")
```

---

### Sec #2: Unchecked File Path in Experiment Runner
**File:** `src/evaluation/ab_testing.py:41`  
**Severity:** LOW

**Issue:**
```python
def _load_dataset(self, path: str) -> List[Dict]:
    with open(path, 'r', encoding='utf-8') as f:
    #  ‚ùå No path validation
    #  ‚ùå Could read any file on system
```

**Risk:** Path traversal if user-controlled

**Fix:**
```python
from pathlib import Path
path = Path(path).resolve()
if not path.is_file():
    raise FileNotFoundError(f"Dataset not found: {path}")
```

---

## ‚ö° PERFORMANCE ISSUES

### Perf #1: Inefficient BM25 Reinitialization
**File:** `src/rag/pipeline.py:138`  
**Severity:** MEDIUM

**Issue:**
```python
# In hybrid_search, BM25 is reinitialized on EVERY query!
all_data = self.vector_store.collection.get(limit=1000)
bm25 = BM25Search(bm25_docs)  # ‚ùå Expensive!
```

**Impact:** 
- Hybrid search 10x slower than necessary
- Tokenization repeated every time

**Fix:**
Cache BM25 instance in RAGPipeline:
```python
def __init__(self):
    self._bm25_cache = None
    
def configure(self, config):
    if config.retrieval_method in ['bm25', 'hybrid']:
        if not self._bm25_cache:
            # Initialize once
            self._bm25_cache = self._init_bm25()
```

---

### Perf #2: Redundant Embedding Generation
**File:** `src/retrieval/vector_search.py:187`  
**Severity:** LOW

**Issue:**
In hybrid_search, semantic search retrieves `n_results * 2` but only uses `n_results`

**Impact:** Waste 50% of embedding compute

**Fix:**
Optimize fusion algorithm to request exactly what's needed

---

## üß™ TESTING GAPS

### Missing Tests:
- [ ] Empty dataset handling
- [ ] Malformed JSON in dataset
- [ ] Zero-score retrieval results
- [ ] API key missing scenario
- [ ] Concurrent experiment runs
- [ ] Large dataset (1000+ questions)
- [ ] Memory leak in long experiments
- [ ] Network failures during LLM calls
- [ ] ChromaDB connection failures

---

## üìã RECOMMENDATIONS

### Immediate Actions:
1. ‚úÖ **Fix Bug #1** (confidence undefined) - BLOCKS ALL TESTING
2. Add dataset validation
3. Add error handling for empty results
4. Test with 5-question subset

### Before Production:
1. Add comprehensive input validation
2. Implement retry logic for LLM failures
3. Add progress persistence (resume interrupted experiments)
4. Add experiment result caching
5. Implement proper logging (not just print)

### Nice to Have:
1. Unit tests for AB testing framework
2. Integration tests for RAG pipeline
3. Load testing for 100+ question datasets
4. Memory profiling for long-running experiments

---

**Next Steps:**
1. Fix critical bugs
2. Test each component in isolation
3. Run integration test with 3 questions
4. Document known limitations

---

**Status:** 7 bugs found (1 critical, 3 medium, 3 low)  
**Estimated Fix Time:** 30 minutes  
**Test Coverage:** 20% ‚Üí Target: 80%
